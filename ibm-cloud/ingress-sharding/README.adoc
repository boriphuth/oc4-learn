=  Ingress Sharding

Explaining how to add additional Ingress controller for sharding Ingress Traffic in IBM Cloud.

The idea is to Split Ingress traffic, Ingress Controller sharding which is useful when balancing incoming traffic load among a set of Ingress Controllers
and then isolating traffic to set of Pods from specific Ingress Controller

* Create Ingress Controller shard
* Create demo applications
* Enable Network polices

== Create Ingress Controller shard

. Get the current dns entries for the cluster for more info refer to
https://cloud.ibm.com/docs/openshift?topic=openshift-loadbalancer_hostname[Classic: Registering a DNS subdomain for an NLB]
+
[source,bash]
----
$ ibmcloud oc nlb-dns ls --cluster myclustername

OK
Hostname                                                                                      IP(s)           Health Monitor   SSL Cert Status   SSL Cert Secret Name                                         Secret Namespace
myclustername-757467-5c19b80d0b42bf06f50309d5c8a080e8-0000.ams03.containers.appdomain.cloud   159.8.232.162   None             created           myclustername-757467-5c19b80d0b42bf06f50309d5c8a080e8-0000   openshift-ingress
----
+
copy the subdomain and add it to the link:sharded-ingress-controller-namespace-selector.yaml[ingress controller yaml file], "domain". But change the 000<n> value in the Hostname to 000<n+1>
+
.sharded-ingress-controller-namespace-selector.yaml
[source,yaml]
----
  apiVersion: operator.openshift.io/v1
  kind: IngressController
  metadata:
    name: sharded-ingress-controller
    namespace: openshift-ingress-operator
  spec:
    replicas: 1
    defaultCertificate:
      name: myclustername-757467-5c19b80d0b42bf06f50309d5c8a080e8-0001 <1>
    domain: myclustername-757467-5c19b80d0b42bf06f50309d5c8a080e8-0001.ams03.containers.appdomain.cloud <2>
    endpointPublishingStrategy:
      loadBalancer:
        scope: External
      type: LoadBalancerService
    nodePlacement:
      tolerations:
      - key: dedicated
        value: edge
    routeSelector:
        matchLabels:
          type: sharded <3>
----
<1> The secret name that will be created by IBM Cloud. So the Pod will keep crashing till secret is created.
<2> Example, the myclustername-757467-5c19b80d0b42bf06f50309d5c8a080e8-0000.ams03.containers.appdomain.cloud subdomain
is changed to myclustername-757467-5c19b80d0b42bf06f50309d5c8a080e8-0001.ams03.containers.appdomain.cloud.
<3> The Ingress Controller selects routes in any namespace that is selected by the route selector that have the label type: sharded
+
NOTE: The 0 value indicates a public subdomain, and the n+1 value indicates the next consecutive subdomain that you create in this cluster.
+
. Add the new Ingress Controller
+
Apply the Ingress Controller router-internal.yaml file:
+
[source,bash]
----
oc apply -f sharded-ingress-controller.yaml

ibmcloud oc nlb-dns create classic --cluster myclustername --type public --secret-namespace openshift-ingress --ip $(oc get svc/router-sharded-ingress-controller -n openshift-ingress -o jsonpath="{.status.loadBalancer.ingress[0].ip}")
----
+
now check if everything is OK
+
[source,bash]
----
ibmcloud oc nlb-dns ls --cluster myclustername
----
+
IMPORTANT: If the output of "ibmcloud oc nlb-dns ls" is Pending Please give it sometime !! it really takes time till the secret is created with certificates !!
+
Once it is create nlb dns entry is created you will be able to resolve it so you can try the following
[source,bash]
----
oc get svc/router-sharded-ingress-controller -n openshift-ingress <1>

nslookup myclustername-757467-5c19b80d0b42bf06f50309d5c8a080e8-0001.ams03.containers.appdomain.cloud <2>
----
<1> Get the EXTERNAL-IP of the newly created service "router-sharded-ingress-controller"
<2> Change the DNS to match your new dns entry, the expected result of the command is the IP of the service ExternalIP

== Create demo applications

So now we will start with creating demo applications.

[source,bash]
----
oc new-project sample1
oc new-app httpd

oc new-project sample2
oc label namespace sample2 type=sharded
oc new-app httpd
----

Now let see the route definition

.route-namespace-selector.yaml
[source,yaml]
----
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  labels:
    app: httpd
    app.kubernetes.io/component: httpd
    app.kubernetes.io/instance: httpd
  name: httpd-shard-1
  namespace: sample2
spec:
  host: httpd-1-sample2.myclustername-757467-5c19b80d0b42bf06f50309d5c8a080e8-0001.ams03.containers.appdomain.cloud <1>
  subdomain: ""
  port:
    targetPort: 8080-tcp
  to:
    kind: Service
    name: httpd
    weight: 100
  wildcardPolicy: None
----
<1> Don't forget to update it to match your DNS entry.

[source,bash]
----
oc apply -f route-namespace-selector.yaml
----

Now lets try to check communication accessibility

[source,bash]
----
curl --max-time 2 http://httpd-1-sample2.myclustername-757467-5c19b80d0b42bf06f50309d5c8a080e8-0001.ams03.containers.appdomain.cloud
----

== Enable Network polices

In the previous section we created a route which is exposed using "sharded-ingress". But by default, the default router have no routeSelector,
and for this reason still we can expose routes using default router

[source,bash]
----
oc expose svc/httpd --hostname=httpd-1-sample2.myclustername-757467-5c19b80d0b42bf06f50309d5c8a080e8-0000.ams03.containers.appdomain.cloud
----

Now lets try to play with curl to check communication accessibility

[source,bash]
----
# The route exposed on default router
curl  --max-time 2 http://httpd-1-sample2.myclustername-757467-5c19b80d0b42bf06f50309d5c8a080e8-0000.ams03.containers.appdomain.cloud

# The route exposed on sharded-ingress router
curl --max-time 2 http://httpd-1-sample2.myclustername-757467-5c19b80d0b42bf06f50309d5c8a080e8-0001.ams03.containers.appdomain.cloud
----

Following documentation https://docs.openshift.com/container-platform/4.3/networking/configuring-networkpolicy.html#nw-networkpolicy-multitenant-isolation_configuring-networkpolicy-plugin[Configuring multitenant isolation using NetworkPolicy]

The following yaml will create multitenant isolation, so pods within sample1 namesapce only are allowed to communicate, and also incoming communication from both default ingress and monitoring

.networkPolicy-default-ingress.yaml
[source, yaml]
----
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: deny-by-default
  namespace: sample1
spec:
  podSelector: {}
  policyTypes:
    - Ingress
---
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-from-openshift-default-ingress
  namespace: sample1
spec:
  ingress:
    - from:
      - namespaceSelector:
          matchLabels:
            network.openshift.io/policy-group: ingress
        podSelector:
          matchLabels:
            ingresscontroller.operator.openshift.io/deployment-ingresscontroller: default
  podSelector: {}
  policyTypes:
  - Ingress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-openshift-monitoring
  namespace: sample1
spec:
  ingress:
    - from:
      - namespaceSelector:
          matchLabels:
            network.openshift.io/policy-group: monitoring
  podSelector: {}
  policyTypes:
  - Ingress
---
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-same-namespace
  namespace: sample1
spec:
  podSelector:
  ingress:
  - from:
    - podSelector: {}
----

[source,bash]
----
oc apply -f networkPolicy-default-ingress.yaml
----

The following yaml will create multitenant isolation, so pods within sample2 namesapce only are allowed to communicate, and also incoming communication from both sharded-ingress-controller and monitoring

.networkPolicy-sharded-ingress.yaml
[source, yaml]
----
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: deny-by-default
  namespace: sample1
spec:
  podSelector: {}
  policyTypes:
    - Ingress
---
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-from-openshift-default-ingress
  namespace: sample1
spec:
  ingress:
    - from:
      - namespaceSelector:
          matchLabels:
            network.openshift.io/policy-group: ingress
        podSelector:
          matchLabels:
            ingresscontroller.operator.openshift.io/deployment-ingresscontroller: sharded-ingress-controller
  podSelector: {}
  policyTypes:
  - Ingress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-openshift-monitoring
  namespace: sample1
spec:
  ingress:
    - from:
      - namespaceSelector:
          matchLabels:
            network.openshift.io/policy-group: monitoring
  podSelector: {}
  policyTypes:
  - Ingress
---
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-same-namespace
  namespace: sample1
spec:
  podSelector:
  ingress:
  - from:
    - podSelector: {}
----
[source,bash]
----
oc apply -f networkPolicy-sharded-ingress.yaml
----

Now lets try again curl to check communication accessibility

[source,bash]
----
# The route exposed on default router
curl  --max-time 2 http://httpd-1-sample2.myclustername-757467-5c19b80d0b42bf06f50309d5c8a080e8-0000.ams03.containers.appdomain.cloud

# The route exposed on sharded-ingress router
curl --max-time 2 http://httpd-1-sample2.myclustername-757467-5c19b80d0b42bf06f50309d5c8a080e8-0001.ams03.containers.appdomain.cloud
----
